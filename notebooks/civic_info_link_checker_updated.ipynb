{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install storysniffer\n",
    "!{sys.executable} -m pip install newspaper3k\n",
    "!{sys.executable} -m pip install -U dill\n",
    "!{sys.executable} -m pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import urllib.parse\n",
    "import hashlib\n",
    "from storysniffer import StorySniffer\n",
    "import ast\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_news_urls(url):\n",
    "    # all URLs of `url`\n",
    "    domain_name = urlparse(url).netloc\n",
    "    urls = set()\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "\n",
    "def crawl(news_url):\n",
    "    print('get link ==> ', news_url)\n",
    "    resp = requests.get(news_url)\n",
    "    return resp.text\n",
    "\n",
    "def get_host(url: str):\n",
    "    token = url.split('://')[1]\n",
    "    token = token.split('.')\n",
    "    if token[0] == 'www':\n",
    "        return token[1]\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "    \n",
    "url = 'http://www.riverfronttimes.com/'\n",
    "\n",
    "urls = get_news_urls(url)\n",
    "\n",
    "valid_urls = []\n",
    "    \n",
    "sniffer = StorySniffer()\n",
    "\n",
    "for u in internal_urls:\n",
    "    try:\n",
    "        if \".pdf\" in u: \n",
    "            continue\n",
    "        if \"/event\" in u: \n",
    "            continue\n",
    "        if \"/places\" in u: \n",
    "            continue\n",
    "        if \"/classifieds\" in u:\n",
    "            continue\n",
    "        if \"/special_sections\" in u:\n",
    "            continue\n",
    "        if \"/form\" in u:\n",
    "            continue\n",
    "        if \"/edition\" in u:\n",
    "            continue\n",
    "        if \"/archive\" in u:\n",
    "            continue\n",
    "        if \"/feed\" in u:\n",
    "            continue\n",
    "        if \"gallery\" in u:\n",
    "            continue\n",
    "        if \"/ads\" in u:\n",
    "            continue\n",
    "        if \"/promo\" in u:\n",
    "            continue\n",
    "        if \"/videos\" in u:\n",
    "            continue\n",
    "        if \"/members-only\" in u:\n",
    "            continue\n",
    "#Video only series from the STL Post-Dispatch\n",
    "        if \"/the-bottom-line\" in u:\n",
    "            continue\n",
    "#Removing the landing pages from the St. Louis Review\n",
    "        if not u[-4:].isnumeric():\n",
    "            continue\n",
    "        if sniffer.guess(u) == True:\n",
    "            valid_urls.append(u)\n",
    "\n",
    "    except BaseException as e:\n",
    "        pass\n",
    "    except SSLError:\n",
    "        pass\n",
    "    except MaxRetryError:\n",
    "        pass\n",
    "    except SSLCertVerificationError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []\n",
    "# print(valid_urls)\n",
    "try:\n",
    "    for u in valid_urls:\n",
    "        hash = hashlib.sha256(u.encode())\n",
    "        news_data.append({\n",
    "            'id': hash.hexdigest(),\n",
    "            'url': u,\n",
    "            'crawl_date': str(datetime.now()),\n",
    "            'text': crawl(u),\n",
    "            'host': urllib.parse.urlsplit(u).hostname\n",
    "            })\n",
    "except BaseException as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles = news_data\n",
    "\n",
    "len(raw_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = raw_articles[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_tags(item,data):\n",
    "    if data['hostname'] == 'missouri.statenews.net' and 'www.kansascity.com':\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(attrs={\"name\":\"keywords\"})\n",
    "        keyword = (key.get(\"content\", None))\n",
    "        tags.append(keyword) \n",
    "        return tags\n",
    "    \n",
    "    elif data['hostname'] == 'www.kansascity.com' or 'www.kmbc.com':\n",
    "        tags = data['meta_data']['keywords']\n",
    "        return tags\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "\n",
    "def given_location(item,data):\n",
    "    if data['hostname'] == 'fox2now.com':\n",
    "        loc = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(attrs={\"name\":\"primaryCategory\"})\n",
    "        # keyword = (key.get(\"content\", None))\n",
    "        loc.append(key)\n",
    "        return loc\n",
    "\n",
    "    \n",
    "def metadata(item,data):\n",
    "    # metadata = data['meta_data']['keywords']\n",
    "    \n",
    "    return metadata\n",
    "    \n",
    "\n",
    "def given_date(item,data):\n",
    "    \n",
    "    if data['hostname'] == 'www.riverfronttimes.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"date\"})\n",
    "            date.append(txt[\"content\"])\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.archstl.org':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\":\"col-12 col-lg-3 ml-auto publish-date-meta text-right\"}).get_text()\n",
    "            txt_cleaned = txt.split('|', 1)[0].replace('Submitted','').replace('\\r','').replace('\\n','').strip()\n",
    "            txt_2 = datetime.strptime(txt_cleaned, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'callnewspapers.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"time-wrapper\"}).get_text().strip('\\n')\n",
    "            txt_cleaned = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com', 'stlamerican.com',\n",
    "                           'www.stltoday.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"dateCreated\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "     \n",
    "    if data['hostname'] == 'sgfneighborhoodnews.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"dtreviewed\"}).get_text().strip('\\n')\n",
    "            txt_cleaned = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'thesalemnewsonline.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"dateCreated\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "   \n",
    "    if data['hostname'] == 'www.columbiamissourian.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"parsely-pub-date\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'missouri.statenews.net':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser') \n",
    "        key = soup.find(attrs={'class':'title_text'})\n",
    "        temp = key.p.get_text(strip=True, separator='\\n').splitlines()\n",
    "        date.append(temp[1])\n",
    "        return date\n",
    "    \n",
    "    elif data['hostname'] == 'www.kansascity.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'publish-date'})\n",
    "        srce = div.contents[0]\n",
    "        date.append(srce)\n",
    "        return date\n",
    "    \n",
    "    if data['hostname'] == 'www.gasconadecountyrepublican.com':\n",
    "        date = data['meta_data']['twt-published-at']\n",
    "        return date\n",
    "    \n",
    "    if data['hostname'] == 'www.hannibal.net':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key  = soup.find('meta', attrs={'name': 'tncms-access-version'})\n",
    "        content = key['content']\n",
    "        date.append(content)\n",
    "        return date\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        date = data['date']\n",
    "        return date\n",
    "    \n",
    "\n",
    "def source(item,data):\n",
    "    if data['hostname'] == 'missouri.statenews.net': \n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser') \n",
    "        key = soup.find(attrs={'class':'title_text'})\n",
    "        temp = key.p.get_text(strip=True, separator='\\n').splitlines()\n",
    "        src.append(temp[0])\n",
    "        return src\n",
    "    \n",
    "    if data['hostname'] == 'abc17news.com':\n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'meta__category'})\n",
    "        srce = div.find('a').contents[0]\n",
    "        src.append(srce)\n",
    "        return src\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "def author(item,data):\n",
    "    \n",
    "    if data['hostname'] == 'www.archstl.org':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        #selector = 'div.col-8 > h3'\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"author-meta d-flex flex-wrap justify-content-center flex-lg-column justify-content-lg-start\"})\n",
    "            auth.append(txt.get_text().replace('\\n','').replace('\\r','').strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.news-leader.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"property\":\"article:author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.semissourian.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':['byline']}).get_text()\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            txt = soup.find('a', {'class':['columnistbyline']}).get_text()\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"reviewer\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'sgfneighborhoodnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"reviewer\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['thesalemnewsonline.com', 'stlamerican.com', 'www.pikecountynews.com', \n",
    "                            'newspressnow.com', 'callnewspapers.com', 'www.stltoday.com', 'www.riverfronttimes.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"author\"})\n",
    "            auth.append(txt[\"content\"].strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'lstribune.net':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt =  soup.find(\"strong\").get_text()\n",
    "            if txt.startswith('By'):\n",
    "                auth.append(txt)\n",
    "                return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.myleaderpaper.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"tnt-byline\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.newstribune.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]    \n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.greenecountycommonwealth.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', class_='field-item even').get_text()\n",
    "            if len(txt) <50:\n",
    "                auth.append(txt)\n",
    "                return auth[0]\n",
    "            else: \n",
    "                return ''\n",
    "            \n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.dddnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'byline'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'dailyjournalonline.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"itemprop\":\"author\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.darnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'byline'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'www.columbiamissourian.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.columbiatribune.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", property=\"article:author\")\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "       \n",
    "    \n",
    "    if data['hostname'] == 'mycameronnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'field field-name-field-dateline field-type-text field-label-inline clearfix'}).get_text().strip('By:\\xa0')\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.cassville-democrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':['byline']}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'carthagenewsonline.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\": \"td-post-author-name\"})\n",
    "            anchor = txt.find('a').get_text()\n",
    "            auth.append(anchor)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'www.californiademocrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('a', attrs={'class':'bi-line-link'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'auroraadvertiser.net':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        spans = soup.find('span', attrs={'class':'byline'})\n",
    "        auth.append(spans.text)\n",
    "\n",
    "    \n",
    "    if data['hostname'] == 'www.bransontrilakesnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", { \"itemprop\" : \"author\" }).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return auth\n",
    "    \n",
    "    if data['hostname'] == 'www.memphisdemocrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        txt_byline = soup.find(attrs={'class':'p2'})\n",
    "        txt = soup.find(\"span\", itemprop=\"name\").text\n",
    "        try:\n",
    "            cleaned = txt_byline.get_text()\n",
    "            if cleaned.startswith('By'): \n",
    "                auth.append(cleaned) \n",
    "                return auth[0]\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.ksdk.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        txt = soup.select_one('span:-soup-contains(\"Author:\")').find_next_sibling(text=True)\n",
    "        aut = txt.strip()\n",
    "        auth.append(aut)\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] == 'www.ksmu.org':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('span', attrs={'class':'ArticlePage-authorBy'})\n",
    "        srce = div.contents[1]\n",
    "        # src.append(srce)\n",
    "        aut = srce.contents[0]\n",
    "        auth.append(aut)\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] == 'dosmundos.com':\n",
    "        auth = data['meta_data']['twitter']['data1']\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] in ['houstonherald.com', 'stljewishlight.org']:\n",
    "        auth= []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"twitter:data1\"})\n",
    "            auth.append(txt[\"content\"].strip())\n",
    "            return auth[0]\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    \n",
    "    if data['hostname'] == 'www.gasconadecountyrepublican.com':\n",
    "        auth = []\n",
    "        aut = data['meta_data']['twt-author-name']\n",
    "        word_list = aut\n",
    "        auth = word_list[3:]\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] == 'www.hannibal.net':\n",
    "        auth = data['meta_data']['author']\n",
    "        return auth\n",
    "\n",
    "    else:\n",
    "        \n",
    "        author = data['author']\n",
    "        return author\n",
    "    \n",
    "def news(item,data):  \n",
    "    \n",
    "    if data['hostname'] == 'www.semissourian.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all('p')\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] == 'www.clintoncountyleader.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"class\" : \"paragraph\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] == 'stlamerican.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        sep = 'TNCMS.AdManager.render'\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text.split(sep, 1)[0])\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''  \n",
    "    \n",
    "\n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com', 'www.columbiamissourian.com', \n",
    "                           'dailyjournalonline.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''    \n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'standard-democrat.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results = soup.find('div',attrs={\"class\":\"text\"})\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            results_paywall = soup.find('div',attrs={\"class\":\"pwblock\"})\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'thesalemnewsonline.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.pikecountynews.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'newspressnow.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] == 'www.clintoncountyleader.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'blog-content'}).get_text()\n",
    "            nws.append(txt)\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "    else:\n",
    "        news = data['news']\n",
    "        return news\n",
    "\n",
    "\n",
    "def metadescription(item,data):\n",
    "    if data['hostname'] == 'dosmundos.com':\n",
    "        desc = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        descrip = soup.find(\"meta\", property=\"og:description\")\n",
    "        des = descrip[\"content\"]\n",
    "        desc.append(des)\n",
    "        return desc\n",
    "    \n",
    "\n",
    "def source_1(item,data):\n",
    "    if data['hostname'] == 'abc17news.com' and 'fox2now.com':\n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'meta__user vcard author'})\n",
    "        srce = div.find('a').contents[0]\n",
    "        src.append(srce)\n",
    "        return src\n",
    "\n",
    "\n",
    "def process_article(item):\n",
    "    article = Article(url=item[\"url\"])\n",
    "    article.download(input_html=item[\"text\"])\n",
    "\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    url = urlparse(item[\"url\"])\n",
    "        # print(hashlib.sha256(item['url'].encode()).hexdigest)\n",
    "    data = {\n",
    "            \"id\": hashlib.sha256(item[\"url\"].encode()).hexdigest(),\n",
    "            \"title\": str(article.title),\n",
    "            \"url\": str(item[\"url\"]),\n",
    "            \"news\": str(article.text),\n",
    "            \"image\": str(article.top_image),\n",
    "            \"date\": str(article.publish_date),\n",
    "            \"author\": article.authors,                                   # author derived from article.authors\n",
    "            \"meta_data\": article.meta_data,\n",
    "            \"hostname\": url.hostname,\n",
    "            \"crawl_datetime\": str(datetime.now()),\n",
    "            \"stats\": {},\n",
    "            \"tags\" : {},                                                   # tags given by the outlet/reporter\n",
    "            \"inferred_tags_set1\" : str(article.keywords),                 # tags derived from article.keywords\n",
    "            \"inferred_tags_set2\" : {},                                    # tags inferred by our custom method\n",
    "        \n",
    "        }\n",
    "    \n",
    "    if len(data[\"news\"]) > 10 and len(data[\"title\"]) > 0:\n",
    "        data['stats']['num_char'] = len(data['news'])\n",
    "        \n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "        # if date =='None':\n",
    "        #     data['date'] = str(article.publish_date)\n",
    "        \n",
    "        tags = given_tags(item,data)\n",
    "        data['tags'] = tags\n",
    "        \n",
    "        src = source(item,data)\n",
    "        data['source'] = src\n",
    "        \n",
    "        src = source_1(item,data)\n",
    "        data['source_1'] = src\n",
    "        \n",
    "        auth = author(item,data)\n",
    "        data['author'] = auth\n",
    "        \n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "\n",
    "    \n",
    "    \n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_articles = []\n",
    "for article in raw_articles: \n",
    "    curated_article = process_article(article)\n",
    "    curated_articles.append(curated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
