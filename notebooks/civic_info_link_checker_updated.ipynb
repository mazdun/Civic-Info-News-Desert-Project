{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install storysniffer\n",
    "!{sys.executable} -m pip install newspaper3k\n",
    "!{sys.executable} -m pip install -U dill\n",
    "!{sys.executable} -m pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import urllib.parse\n",
    "import hashlib\n",
    "from storysniffer import StorySniffer\n",
    "import ast\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_news_urls(url):\n",
    "    \n",
    "    # all URLs of `url` \n",
    "    domain_name = urlparse(url).netloc\n",
    "    urls = set()\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko; Google Page Speed Insights) Chrome/41.0.2272.118 Safari/537.36'})\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)      \n",
    "        \n",
    "def crawl(news_url):\n",
    "    print('get link ==> ', news_url)\n",
    "    resp = requests.get(news_url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko; Google Page Speed Insights) Chrome/41.0.2272.118 Safari/537.36'})\n",
    "    return resp.text\n",
    "\n",
    "def get_host(url: str):\n",
    "    token = url.split('://')[1]\n",
    "    token = token.split('.')\n",
    "    if token[0] == 'www':\n",
    "        return token[1]\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "    \n",
    "url = 'https://www.studlife.com/news/'\n",
    "\n",
    "urls = get_news_urls(url)\n",
    "\n",
    "valid_urls = []\n",
    "    \n",
    "sniffer = StorySniffer()\n",
    "\n",
    "for u in internal_urls:\n",
    "    try:\n",
    "        if \".pdf\" in u: \n",
    "            continue\n",
    "        if \"/event\" in u: \n",
    "            continue\n",
    "        if \"/places\" in u: \n",
    "            continue\n",
    "        if \"/classifieds\" in u:\n",
    "            continue\n",
    "        if \"/special_sections\" in u:\n",
    "            continue\n",
    "        if \"/form\" in u:\n",
    "            continue\n",
    "        if \"/edition\" in u:\n",
    "            continue\n",
    "        if \"/archive\" in u:\n",
    "            continue\n",
    "        if \"/feed\" in u:\n",
    "            continue\n",
    "        if \"gallery\" in u:\n",
    "            continue\n",
    "        if \"/ads\" in u:\n",
    "            continue\n",
    "        if \"/promo\" in u:\n",
    "            continue\n",
    "        if \"/video\" in u:\n",
    "            continue\n",
    "        if \"/slideshow\" in u:\n",
    "            continue\n",
    "        if \"/members-only\" in u:\n",
    "            continue\n",
    "#Video only series from the STL Post-Dispatch\n",
    "        if \"/the-bottom-line\" in u:\n",
    "            continue\n",
    "#Removing non-article pages from the Suburban Journals of Greater St. Louis\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.stltoday.com') and (not '/article' in u):\n",
    "            continue\n",
    "#Removing the landing pages from the St. Louis Review\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.archstl.org') and (not u[-4:].isnumeric()):\n",
    "            continue\n",
    "#Removing landing pages from the Sullivan Independent News\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.mysullivannews.com') and (\"bourbon\" in u):\n",
    "            continue\n",
    "#Removing landing pages from The Fulton Sun\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.fultonsun.com') and (('/fulton-news/' in u) or (not '-' in u)):\n",
    "            continue\n",
    "#Removing image only pages from The Joplin Globe\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.joplinglobe.com') and ('/image' in u):\n",
    "            continue\n",
    "#Removing calendar pages from The Marshall Democrat-News\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.marshallnews.com') and ('-calendar' in u):\n",
    "            continue\n",
    "#Removing landing pages from the Maryville Forum\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.maryvilleforum.com') and (('/multimedia/photos' in u) or (not '.html' in u)):\n",
    "            continue\n",
    "#Removing landing pages from The Pitch\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.thepitchkc.com') and (('/city-rag-kansas-city-news' in u) or ('/dessert-week-2022' in u)):\n",
    "            continue\n",
    "#Removing advertising pages from The Standard\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.the-standard.org') and ('/advertise' in u):\n",
    "            continue\n",
    "#Removing landing pages from the Webster Kirkwood News\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'websterkirkwoodtimes.com') and (not '/article' in u):\n",
    "            continue\n",
    "#Removing landing pages from the West End Word\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.timesnewspapers.com') and (not '/article' in u):\n",
    "            continue\n",
    "#Removing landing pages from KRZK\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.legends1063.fm') and ('/advertise-with-us' in u):\n",
    "            continue\n",
    "#Removing landing pages from KCUR and KSMU\n",
    "        if (urllib.parse.urlsplit(u).hostname in ['www.kcur.org', 'www.ksmu.org', 'www.krcu.org']) and (re.findall(r'\\d{4}-\\d{2}-\\d{2}', u) == []):\n",
    "            continue\n",
    "#Removing landing pages from News Press Now\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.newspressnow.com') and (not '.html' in u):\n",
    "            continue\n",
    "#Removing landing pages from KTTS\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.ktts.com') and (not '-' in u):\n",
    "            continue\n",
    "#Removing landing pages from KPRS\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.kprs.com') and (re.findall(r'\\d{4}', u) == []):\n",
    "            continue\n",
    "#Removing landing pages from My MO Info\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.mymoinfo.com') and ('/trading-post' in u):\n",
    "            continue\n",
    "#Removing landing pages from KBIA\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.kbia.org') and (('/show' in u) or ('/podcast' in u) or ('paul-pepper' in u) or (re.findall(r'\\d{4}-\\d{2}-\\d{2}', u) == [])):\n",
    "            continue\n",
    "#Removing podcast landing pages from 97.1FM and 98.1FM\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.audacy.com') and (('/podcast' in u) or ('/author' in u)):\n",
    "            continue\n",
    "#Removing non-news from The Beat\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.digitalburg.com') and (not '/central-news' in u):\n",
    "            continue\n",
    "#Removing non-news from Farmer Publishing\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'farmerpublishing.com') and ('e-mail-newsletter' in u):\n",
    "            continue\n",
    "#Removing landing pages from Benton County Enterprise\n",
    "        if (urllib.parse.urlsplit(u).hostname in ['www.bentoncountyenterprise.com']) and (not re.findall(r'\\d{4}', u) == []):\n",
    "            continue\n",
    "        if (urllib.parse.urlsplit(u).hostname in ['lewispnj.com', 'maconhomepress.com', 'www.theijnews.com']) and ('/articles' in u):\n",
    "            valid_urls.append(u)\n",
    "        if (urllib.parse.urlsplit(u).hostname == 'www.thecash-book.com') and (len(u)<55):\n",
    "            continue\n",
    "        if (urllib.parse.urlsplit(u).hostname in ['www.hermannadvertisercourier.com', 'www.lakegazette.net']) and (re.findall(r'\\d', u) == []):\n",
    "            continue\n",
    "        if (urllib.parse.urlsplit(u).hostname in ['bocojo.com']) and (('/news' in u) or ('/obituaries' in u) or ('/sports' in u) or ('/opinion' in u)) and (not '/category' in u):\n",
    "            valid_urls.append(u)    \n",
    "        if (urllib.parse.urlsplit(u).hostname in ['dosmundos.com', 'newstalkkzrg.com', 'www.949kcmo.com']) and (re.findall(r'\\d{4}\\/', u) == []):\n",
    "            continue \n",
    "        if (urllib.parse.urlsplit(u).hostname in ['www.redlatinastl.com']) and ('/noticia' in u):\n",
    "            valid_urls.append(u)   \n",
    "        if (urllib.parse.urlsplit(u).hostname in ['emissourian.com']) and (('/marketplace' in u) or (re.findall(r'\\d', u) == [])):\n",
    "            continue \n",
    "        if (urllib.parse.urlsplit(u).hostname in ['molawyersmedia.com', 'store.molawyersmedia.com']) and (('/submit' in u) or ('store.' in u) or (re.findall(r'\\d{4}\\/', u) == [])):\n",
    "            continue \n",
    "            \n",
    "        if sniffer.guess(u) == True and (not u in valid_urls):\n",
    "            valid_urls.append(u)\n",
    "\n",
    "    except BaseException as e:\n",
    "        pass\n",
    "    except SSLError:\n",
    "        pass\n",
    "    except MaxRetryError:\n",
    "        pass\n",
    "    except SSLCertVerificationError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []\n",
    "# print(valid_urls)\n",
    "try:\n",
    "    for u in valid_urls:\n",
    "        hash = hashlib.sha256(u.encode())\n",
    "        news_data.append({\n",
    "            'id': hash.hexdigest(),\n",
    "            'url': u,\n",
    "            'crawl_date': str(datetime.now()),\n",
    "            'text': crawl(u),\n",
    "            'host': urllib.parse.urlsplit(u).hostname\n",
    "            })\n",
    "except BaseException as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles = news_data\n",
    "\n",
    "len(raw_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = raw_articles[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_tags(item,data):\n",
    "    \n",
    "    if data['hostname'] in ['www.bethanyclipper.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(\"span\", attrs={\"class\":\"tag-links\"})\n",
    "        try:\n",
    "            for x in key.find_all('a'):\n",
    "                tags.append(x.get_text())\n",
    "            return tags\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.excelsiorspringsstandard.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(\"ul\", attrs={\"class\":\"links field__items\"})\n",
    "        tag = key.find('a').get_text()\n",
    "        tags.append(tag) \n",
    "        return [tags[0]]\n",
    "    \n",
    "    if data['hostname'] in ['www.dixonpilot.com', 'www.edinasentinel.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(\"div\", attrs={\"class\":\"fl-post-cats-tags\"})\n",
    "        tag = key.find('a').get_text()\n",
    "        tags.append(tag) \n",
    "        return [tags[0]]\n",
    "    \n",
    "    if data['hostname'] in ['missouri.statenews.net', 'www.kansascity.com', 'www.kxcv.org']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(attrs={\"name\":\"keywords\"})\n",
    "        keyword = (key.get(\"content\", None))\n",
    "        tags.append(keyword) \n",
    "        return tags\n",
    "    \n",
    "    if data['hostname'] == 'www.949kcmo.com':\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find_all(\"meta\", attrs={\"property\":\"article:section\"})\n",
    "            for x in key:\n",
    "                keyword = (x.get(\"content\", None))\n",
    "                tags.append(keyword) \n",
    "            return tags\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'unewsonline.com':\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"meta\", attrs={\"property\":\"article:section\"})\n",
    "            keyword = (key.get(\"content\", None))\n",
    "            tags.append(keyword) \n",
    "            return tags[0]\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['molawyersmedia.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"meta\", attrs={\"name\":\"keywords\"})\n",
    "            keyword = (key.get(\"content\", None))\n",
    "            keyword2 = keyword.split(\",\")\n",
    "            tags=keyword2 \n",
    "            return tags\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.hermannadvertisercourier.com', 'www.lakegazette.net', 'emissourian.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"meta\", attrs={\"name\":\"news_keywords\"})\n",
    "            keyword = (key.get(\"content\", None))\n",
    "            keyword2 = keyword.replace(',','').split(\" \")\n",
    "            tags=keyword2 \n",
    "            return tags\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['dailyjournalonline.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"meta\", attrs={\"name\":\"news_keywords\"})\n",
    "            keyword = (key.get(\"content\", None))\n",
    "            tags.append(keyword) \n",
    "            return tags\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.audacy.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find_all(\"meta\", attrs={\"name\":\"branch:deeplink:editorial_tags\"})\n",
    "            for x in key:\n",
    "                keyword = (x.get(\"content\", None))\n",
    "                tags.append(keyword) \n",
    "            return tags\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except IndexError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.kprs.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"a\", attrs={\"rel\":\"category tag\"}).get_text()\n",
    "            tags.append(key) \n",
    "            return tags[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except IndexError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.kcur.org', 'news.stlpublicradio.org']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find_all(\"meta\", attrs={\"property\":\"article:tag\"})\n",
    "            for x in key:\n",
    "                keyword = (x.get(\"content\", None))\n",
    "                tags.append(keyword) \n",
    "            return tags\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except IndexError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.krcu.org']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find_all(\"meta\", attrs={\"property\":\"article:tag\"})\n",
    "            for x in key:\n",
    "                keyword = (x.get(\"content\", None))\n",
    "                tags.append(keyword) \n",
    "            return tags[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except IndexError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['www.newspressnow.com']:\n",
    "        tags = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"div\", attrs={\"class\":\"asset-tags hidden-print\"})\n",
    "            for x in key.find_all('a'):\n",
    "                keyword = (x.get_text())\n",
    "                tags.append(keyword) \n",
    "            return tags\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except IndexError:\n",
    "            return ''\n",
    "    \n",
    "    elif data['hostname'] == 'www.kansascity.com' or 'www.kmbc.com':\n",
    "        tags = data['meta_data']['keywords']\n",
    "        return tags\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "\n",
    "def given_location(item,data):\n",
    "    if data['hostname'] == 'fox2now.com':\n",
    "        loc = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key = soup.find(attrs={\"name\":\"primaryCategory\"})\n",
    "        # keyword = (key.get(\"content\", None))\n",
    "        loc.append(key)\n",
    "        return loc\n",
    "\n",
    "    \n",
    "def metadata(item,data):\n",
    "    # metadata = data['meta_data']['keywords']\n",
    "    \n",
    "    return metadata\n",
    "    \n",
    "\n",
    "def given_date(item,data):\n",
    "    \n",
    "    if data['hostname'] in ['www.redlatinastl.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"li\", attrs={\"class\":\"text-muted\"}).get_text().strip()\n",
    "            txt_2 = datetime.strptime(txt, '%Y-%m-%d')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['cedarrepublican.com', 'ccheadliner.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", attrs={\"class\":\"monthday\"}).get_text().strip()\n",
    "            txt_2 = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.bethanyclipper.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"col-12 col-sm contact-section\"}).get_text().replace('\\n','')\n",
    "            txt_2 = datetime.strptime(txt, '%A, %B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['www.mycaldwellcounty.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", attrs={\"property\":\"dc:date dc:created\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['moberlymonitor.com', 'gasconadecountyrepublican.com', 'www.kirksvilledailyexpress.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", {\"name\":\"twt-published-at\"})\n",
    "            txt_date = txt.get(\"content\", None)[:-6]\n",
    "            #Tue, 11 Oct 2022 12:00:00\n",
    "            txt_2 = datetime.strptime(txt_date, '%a, %d %b %Y %H:%M:%S')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.kxcv.org']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"p\", {\"class\":\"date\"})\n",
    "            txt_date = txt.find('b').get_text().replace('Sept.','Sep').replace('Jan.','Jan').replace('Feb.','Feb').replace('March','Mar').replace('April','Apr').replace('June','Jun').replace('July','Jul').replace('Oct.','Oct').replace('Nov.','Nov').replace('Dec.', 'Dec')\n",
    "            txt_2 = datetime.strptime(txt_date, '%b %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.theijnews.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\":\"published-date\"}).get_text().strip().replace(',','')\n",
    "            #September 29, 2022\n",
    "            txt_2 = datetime.strptime(txt, '%B %d %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['lewispnj.com','maconhomepress.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\":\"published-date\"}).get_text().strip()\n",
    "            #September 29, 2022\n",
    "            txt_2 = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] == 'www.firesideguard.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\":\"pss-dateline\"}).get_text().replace('Sunday,','').replace('Monday,','').replace('Tuesday,','').replace('Wednesday,','').replace('Thursday,','').replace('Friday,','').replace('Saturday,','').strip().replace('th','').replace('nd','').replace('rd','').replace('st','')\n",
    "            #October 9, 2022\n",
    "            txt_2 = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.legends1063.fm':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"p\", {\"class\":\"news-timestamp\"}).get_text().strip().replace('th','').replace('nd','').replace('rd','').replace('st','').replace('am','AM').replace('pm','PM')\n",
    "            #Thursday, September 22, 2022  6:21\n",
    "            txt_2 = datetime.strptime(txt, '%A, %B %d, %Y  %I:%M%p')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] == 'www.4bcaonline.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            key = soup.find(\"meta\", attrs={\"property\":\"og:description\"})\n",
    "            date_text = (key.get(\"content\", None))\n",
    "            txt_2 = datetime.strptime(date_text, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'unewsonline.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"time-wrapper\"}).get_text()\n",
    "            txt_2 = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.theccreporter.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"p\", string=re.compile(r', \\d\\d\\d\\d'))\n",
    "            txt_cleaned = txt.get_text().replace('Published ','').replace('\\n','').strip('\\r')\n",
    "            txt_2 = datetime.strptime(txt_cleaned, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''   \n",
    "    \n",
    "    if data['hostname'] == 'themaneater.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"p\", attrs={\"class\":\"post-date\"}).get_text()\n",
    "            txt_cleaned = txt.replace('th','').replace('nd','').replace('rd','').replace('st','')\n",
    "            txt_2 = datetime.strptime(txt_cleaned, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.riverfronttimes.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"date\"})\n",
    "            date.append(txt[\"content\"])\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.archstl.org':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\":\"col-12 col-lg-3 ml-auto publish-date-meta text-right\"}).get_text()\n",
    "            txt_cleaned = txt.split('|', 1)[0].replace('Submitted','').replace('\\r','').replace('\\n','').strip()\n",
    "            txt_2 = datetime.strptime(txt_cleaned, '%B %d, %Y')\n",
    "            date.append(str(txt_2))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'callnewspapers.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"time-wrapper\"}).get_text().strip('\\n')\n",
    "            txt_cleaned = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] in ['www.bentoncountyenterprise.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", attrs={\"property\":\"dc:date dc:created\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['www.plattecountycitizen.com', 'the-standard.org']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"datePublished\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com', 'stlamerican.com',\n",
    "                           'www.stltoday.com', 'www.joplinglobe.com', 'www.maryvilleforum.com', 'websterkirkwoodtimes.com',\n",
    "                           'www.newspressnow.com', 'dailyjournalonline.com', 'www.hermannadvertisercourier.com',\n",
    "                           'www.lakegazette.net', 'emissourian.com']:\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"dateCreated\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "     \n",
    "    if data['hostname'] == 'sgfneighborhoodnews.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"dtreviewed\"}).get_text().strip('\\n')\n",
    "            txt_cleaned = datetime.strptime(txt, '%B %d, %Y')\n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        except ValueError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'thesalemnewsonline.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"dateCreated\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "   \n",
    "    if data['hostname'] == 'www.columbiamissourian.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"parsely-pub-date\"})\n",
    "            txt_cleaned = pd.to_datetime(txt[\"content\"]).tz_convert('utc').tz_localize(None) \n",
    "            date.append(str(txt_cleaned))\n",
    "            return date[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'missouri.statenews.net':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser') \n",
    "        key = soup.find(attrs={'class':'title_text'})\n",
    "        temp = key.p.get_text(strip=True, separator='\\n').splitlines()\n",
    "        date.append(temp[1])\n",
    "        return date\n",
    "    \n",
    "    elif data['hostname'] == 'www.kansascity.com':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'publish-date'})\n",
    "        srce = div.contents[0]\n",
    "        date.append(srce)\n",
    "        return date\n",
    "    \n",
    "    if data['hostname'] == 'www.gasconadecountyrepublican.com':\n",
    "        date = data['meta_data']['twt-published-at']\n",
    "        return date\n",
    "    \n",
    "    if data['hostname'] == 'www.hannibal.net':\n",
    "        date = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        key  = soup.find('meta', attrs={'name': 'tncms-access-version'})\n",
    "        content = key['content']\n",
    "        date.append(content)\n",
    "        return date\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        date = data['date']\n",
    "        return date\n",
    "    \n",
    "\n",
    "def source(item,data):\n",
    "    if data['hostname'] == 'missouri.statenews.net': \n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser') \n",
    "        key = soup.find(attrs={'class':'title_text'})\n",
    "        temp = key.p.get_text(strip=True, separator='\\n').splitlines()\n",
    "        src.append(temp[0])\n",
    "        return src\n",
    "    \n",
    "    if data['hostname'] == 'abc17news.com':\n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'meta__category'})\n",
    "        srce = div.find('a').contents[0]\n",
    "        src.append(srce)\n",
    "        return src\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "def author(item,data):\n",
    "    \n",
    "    if data['hostname'] in ['bocojo.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"c5-white-box layout-2 c5-margin clearfix\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                if y.get_text().startswith('By:'):\n",
    "                    auth.append(y.get_text())\n",
    "            return ' '.join(auth).replace('By:','').strip()\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['ccheadliner.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"twt-author-name\"})\n",
    "            txt2 = (txt.get(\"content\", None)).replace('By','').strip().split(\"\\n\",1)[0].lower()\n",
    "            auth.append(txt2)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] in ['moberlymonitor.com', 'cedarrepublican.com', 'gasconadecountyrepublican.com', \n",
    "                            'www.kirksvilledailyexpress.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"twt-author-name\"})\n",
    "            txt2 = (txt.get(\"content\", None)).split('\\n',1)[0].replace('By','').strip()\n",
    "            auth.append(txt2)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] in ['maconhomepress.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"author\"})\n",
    "            x = txt.get_text().replace('by','').strip()\n",
    "            auth.append(x)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] in ['www.mycaldwellcounty.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"property\":\"content:encoded\"})\n",
    "            x = txt.find('a').get_text()\n",
    "            auth.append(x)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] in ['www.excelsiorspringsstandard.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"user-name\"})\n",
    "            x = txt.find('a').get_text()\n",
    "            auth.append(x)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] in ['www.dixonpilot.com', 'www.edinasentinel.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", attrs={\"itemprop\":\"name\"}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] in ['www.kxcv.org']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"i\").get_text().encode('ascii', 'ignore').decode('ascii')\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] in ['www.kprs.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", attrs={\"class\":\"single-meta\"}).get_text()\n",
    "            txt2 = txt.replace('|','').strip()\n",
    "            auth.append(txt2)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] in ['www.kbia.org']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"gtm-dataLayer\"})\n",
    "            txt2 = txt[\"content\"].strip()\n",
    "            txt_dict = json.loads(txt2)\n",
    "            auth.append(txt_dict['gtmAuthor'].strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''  \n",
    "    \n",
    "    if data['hostname'] in ['www.kcur.org', 'news.stlpublicradio.org', 'www.ksmu.org', 'www.krcu.org']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"gtm-dataLayer\"})\n",
    "            txt2 = txt[\"content\"].strip()\n",
    "            txt_dict = json.loads(txt2)\n",
    "            auth.append(txt_dict['gtmAuthor'].strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''   \n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.legends1063.fm':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"p\", attrs={\"class\":\"news-author\"})\n",
    "            auth.append(txt.get_text().replace('By','').replace('\\n','').strip().split('  ', 1)[0])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'themaneater.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"p\", attrs={\"class\":\"post-author\"})\n",
    "            auth.append(txt.get_text().strip('By '))\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.southeastarrow.com', 'www.nevadadailymail.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"byline\"})\n",
    "            auth.append(txt.get_text())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.archstl.org':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        #selector = 'div.col-8 > h3'\n",
    "        try:\n",
    "            txt = soup.find(\"div\", attrs={\"class\":\"author-meta d-flex flex-wrap justify-content-center flex-lg-column justify-content-lg-start\"})\n",
    "            auth.append(txt.get_text().replace('\\n','').replace('\\r','').strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.news-leader.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"property\":\"article:author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.semissourian.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':['byline']}).get_text()\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            txt = soup.find('a', {'class':['columnistbyline']}).get_text()\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"reviewer\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'sgfneighborhoodnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"reviewer\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['thesalemnewsonline.com', 'stlamerican.com', 'www.pikecountynews.com', \n",
    "                            'newspressnow.com', 'www.newspressnow.com', 'callnewspapers.com', 'www.stltoday.com', 'www.riverfronttimes.com',\n",
    "                           'www.joplinglobe.com', 'websterjournal.com', 'plattecountylandmark.com', \n",
    "                           'www.maryvilleforum.com', 'the-standard.org', 'unewsonline.com', 'websterkirkwoodtimes.com', \n",
    "                           'www.thecash-book.com', 'dailyjournalonline.com', 'www.hermannadvertisercourier.com',\n",
    "                           'www.lakegazette.net', 'emissourian.com', 'molawyersmedia.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"author\"})\n",
    "            auth.append(txt[\"content\"].strip())\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'lstribune.net':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt =  soup.find(\"strong\").get_text()\n",
    "            if txt.startswith('By'):\n",
    "                auth.append(txt)\n",
    "                return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.myleaderpaper.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"class\":\"tnt-byline\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.newstribune.com', 'www.fultonsun.com', 'www.plattecountycitizen.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"itemprop\":\"author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]    \n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.greenecountycommonwealth.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', class_='field-item even').get_text()\n",
    "            if len(txt) <50:\n",
    "                auth.append(txt)\n",
    "                return auth[0]\n",
    "            else: \n",
    "                return ''\n",
    "            \n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.dddnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'byline'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'dailyjournalonline.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", {\"itemprop\":\"author\"}).get_text().strip()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.darnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'byline'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'www.columbiamissourian.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"author\"})\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'www.audacy.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", property=\"article:author:name\")\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.columbiatribune.com', 'newstalkkzrg.com']:\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", property=\"article:author\")\n",
    "            auth.append(txt[\"content\"])\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "       \n",
    "    \n",
    "    if data['hostname'] == 'mycameronnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'field field-name-field-dateline field-type-text field-label-inline clearfix'}).get_text().strip('By:\\xa0')\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.cassville-democrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':['byline']}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'carthagenewsonline.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"div\", {\"class\": \"td-post-author-name\"})\n",
    "            anchor = txt.find('a').get_text()\n",
    "            auth.append(anchor)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'www.californiademocrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('a', attrs={'class':'bi-line-link'}).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'auroraadvertiser.net':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        spans = soup.find('span', attrs={'class':'byline'})\n",
    "        auth.append(spans.text)\n",
    "\n",
    "    \n",
    "    if data['hostname'] == 'www.bransontrilakesnews.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find(\"span\", { \"itemprop\" : \"author\" }).get_text()\n",
    "            auth.append(txt)\n",
    "            return auth[0]\n",
    "\n",
    "        except AttributeError:\n",
    "            return auth\n",
    "    \n",
    "    if data['hostname'] == 'www.memphisdemocrat.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        txt_byline = soup.find(attrs={'class':'p2'})\n",
    "        txt = soup.find(\"span\", itemprop=\"name\").text\n",
    "        try:\n",
    "            cleaned = txt_byline.get_text()\n",
    "            if cleaned.startswith('By'): \n",
    "                auth.append(cleaned) \n",
    "                return auth[0]\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.ksdk.com':\n",
    "        auth = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        txt = soup.select_one('span:-soup-contains(\"Author:\")').find_next_sibling(text=True)\n",
    "        aut = txt.strip()\n",
    "        auth.append(aut)\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] == 'dosmundos.com':\n",
    "        auth = []\n",
    "        try:\n",
    "            soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "            results=soup.find(\"div\", {\"class\" : \"elementor-element elementor-element-18433643 elementor-widget elementor-widget-theme-post-content\"}).get_text()\n",
    "            x = re.findall(r'(By\\s\\w+\\s[A-Z].*?(?=[A-Z]))', results)\n",
    "            auth.append(x)\n",
    "            y = auth[0]\n",
    "            return ' '.join(y).split('\\n',1)[0].split('and',1)[0].replace('By','').strip()\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['houstonherald.com', 'stljewishlight.org', 'www.marshallnews.com', \n",
    "                           'www.monett-times.com', 'mbcpathway.com', 'www.thepitchkc.com', \n",
    "                           'www.mymoinfo.com']:\n",
    "        auth= []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        try:\n",
    "            txt = soup.find(\"meta\", attrs={\"name\":\"twitter:data1\"})\n",
    "            auth.append(txt[\"content\"].strip())\n",
    "            return auth[0]\n",
    "           \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        except TypeError:\n",
    "            return ''\n",
    "\n",
    "    \n",
    "    if data['hostname'] == 'www.gasconadecountyrepublican.com':\n",
    "        auth = []\n",
    "        aut = data['meta_data']['twt-author-name']\n",
    "        word_list = aut\n",
    "        auth = word_list[3:]\n",
    "        return auth\n",
    "    \n",
    "    if data['hostname'] == 'www.hannibal.net':\n",
    "        auth = data['meta_data']['author']\n",
    "        return auth\n",
    "\n",
    "    else:\n",
    "        \n",
    "        author = data['author']\n",
    "        return author\n",
    "    \n",
    "def news(item,data): \n",
    "    \n",
    "    if data['hostname'] in ['molawyersmedia.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"entry entry-content\"})\n",
    "            y = results.find_all('p')\n",
    "            for z in y:\n",
    "                nws.append(z.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','')    \n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['dosmundos.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"elementor-element elementor-element-18433643 elementor-widget elementor-widget-theme-post-content\"})\n",
    "            y = results.find('strong').get_text()\n",
    "            z = re.sub(r'(By\\s\\w+\\s[A-Z].*?(?=[A-Z]))', '', y)\n",
    "            return ''.join(z)    \n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.redlatinastl.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"section\", {\"class\" : \"blog_area single-post-area section-padding\"})\n",
    "            y = results.find_all('p')\n",
    "            for z in y:\n",
    "                nws.append(z.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','')    \n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['cedarrepublican.com', 'ccheadliner.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"body main-body clearfix\"})\n",
    "            x = results.find_all('p', {\"class\":\"p1\"})\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','')\n",
    "            \n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['bocojo.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"c5-white-box layout-2 c5-margin clearfix\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                if y.get_text().startswith('By:'):\n",
    "                    continue\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','').split('See more in', 1)[0].strip()\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        except TypeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.bethanyclipper.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"entry-content\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','').split('How useful', 1)[0].strip()\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['moberlymonitor.com', 'gasconadecountyrepublican.com', 'www.kirksvilledailyexpress.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"body main-body clearfix\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','')\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.dixonpilot.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"itemprop\" : \"text\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','')\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['www.edinasentinel.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"itemprop\" : \"text\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').replace('\\n','').replace('If youre a current subscriber, log in below. If you would like to subscribe, please click the subscribe tab above.  Username    Password       Login   Lost your password?  Please enter your email and we will send your username and password to you.  Email    Submit   Back ','').strip()\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['dailyjournalonline.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            x = results.find_all('p')\n",
    "            for y in x:\n",
    "                nws.append(y.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').split('  Get', 1)[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.ccscrnews.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"class\" : \"paragraph\"})\n",
    "            for x in results:\n",
    "                nws.append(x.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').split(\"All content copyright\", 1)[0].replace('Braden Clines','').replace('Kadee Brosseau DeCourley','').strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] == 'www.4bcaonline.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"class\" : \"paragraph\"})\n",
    "            for x in results:\n",
    "                nws.append(x.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii').split(\" Barry County AdvertiserOffice:\", 1)[0].replace('Adriana Keeton','')\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "        \n",
    "    if data['hostname'] in ['lewispnj.com', 'maconhomepress.com','www.theijnews.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"article-main\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii')\n",
    "            \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] == 'www.ktts.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"jeg-content-inner\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii')\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] == 'www.nevadadailymail.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"content\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii')\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] == 'themaneater.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"wp-container-2 wp-block-column\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws)\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'plattecountylandmark.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"content-inner\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws)\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.southeastarrow.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"class\" : \"text\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "#.encode('ascii', 'ignore').decode('ascii') removes the \\x91, etc. characters\n",
    "            return ' '.join(nws).encode('ascii', 'ignore').decode('ascii')\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] == 'www.semissourian.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all('p')\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] == 'www.clintoncountyleader.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"class\" : \"paragraph\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "            \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] == 'stlamerican.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        sep = 'TNCMS.AdManager.render'\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text.split(sep, 1)[0])\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "\n",
    "    if data['hostname'] == 'www.joplinglobe.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        sep = ';\\n\\n'\n",
    "        sep2 = '\\n\\n\\n'\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text.split(sep, 1)[1].split(sep, 1)[1].split(sep2, 1)[0])\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return '' \n",
    "    \n",
    "    if data['hostname'] in ['www.hermannadvertisercourier.com','www.lakegazette.net']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws).replace('\\n','')\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "\n",
    "    if data['hostname'] in ['www.timesnewspapers.com','www.bransontrilakesnews.com', 'www.columbiamissourian.com', \n",
    "                           'dailyjournalonline.com', 'www.maryvilleforum.com', 'websterkirkwoodtimes.com', \n",
    "                           'emissourian.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text.replace('\\n',''))\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] in ['the-standard.org']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0].encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''          \n",
    "    \n",
    "    if data['hostname'] == 'standard-democrat.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results = soup.find('div',attrs={\"class\":\"text\"})\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            results_paywall = soup.find('div',attrs={\"class\":\"pwblock\"})\n",
    "            nws.append(results.get_text())\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "\n",
    "    if data['hostname'] == 'thesalemnewsonline.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeErrdeor:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    if data['hostname'] == 'www.pikecountynews.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find_all(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for res in results:\n",
    "                nws.append(res.text)\n",
    "                return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "    if data['hostname'] in ['newspressnow.com']:\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            results=soup.find(\"div\", {\"itemprop\" : \"articleBody\"})\n",
    "            for p in results.find_all('p'):\n",
    "                nws.append(p.get_text())\n",
    "            return ' '.join(nws)\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "    if data['hostname'] == 'www.clintoncountyleader.com':\n",
    "        nws = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('div', {'class':'blog-content'}).get_text()\n",
    "            nws.append(txt)\n",
    "            return nws[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "        \n",
    "    else:\n",
    "        news = data['news']\n",
    "        return news\n",
    "\n",
    "\n",
    "def metadescription(item,data):\n",
    "    if data['hostname'] == 'dosmundos.com':\n",
    "        desc = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        descrip = soup.find(\"meta\", property=\"og:description\")\n",
    "        des = descrip[\"content\"]\n",
    "        desc.append(des)\n",
    "        return desc\n",
    "    \n",
    "def title(item,data):\n",
    "    if data['hostname'] == 'www.firesideguard.com':\n",
    "        title = []\n",
    "        soup = BeautifulSoup(article['text'], \"html.parser\")\n",
    "        try:\n",
    "            txt = soup.find('h1').get_text()\n",
    "            title.append(txt)\n",
    "            return title[0]\n",
    "        \n",
    "        except AttributeError:\n",
    "            return ''\n",
    "    \n",
    "\n",
    "def source_1(item,data):\n",
    "    if data['hostname'] == 'abc17news.com' and 'fox2now.com':\n",
    "        src = []\n",
    "        soup = BeautifulSoup(article['text'],'html.parser')\n",
    "        div = soup.find('div', attrs={'class':'meta__user vcard author'})\n",
    "        srce = div.find('a').contents[0]\n",
    "        src.append(srce)\n",
    "        return src\n",
    "\n",
    "news\n",
    "def process_article(item):\n",
    "    article = Article(url=item[\"url\"])\n",
    "    article.download(input_html=item[\"text\"])\n",
    "\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    url = urlparse(item[\"url\"])\n",
    "        # print(hashlib.sha256(item['url'].encode()).hexdigest)\n",
    "    data = {\n",
    "            \"id\": hashlib.sha256(item[\"url\"].encode()).hexdigest(),\n",
    "            \"title\": str(article.title),\n",
    "            \"url\": str(item[\"url\"]),\n",
    "            \"news\": str(article.text),\n",
    "            \"image\": str(article.top_image),\n",
    "            \"date\": str(article.publish_date),\n",
    "            \"author\": article.authors,                                   # author derived from article.authors\n",
    "            \"meta_data\": article.meta_data,\n",
    "            \"hostname\": url.hostname,\n",
    "            \"crawl_datetime\": str(datetime.now()),\n",
    "            \"stats\": {},\n",
    "            \"tags\" : {},                                                   # tags given by the outlet/reporter\n",
    "            \"inferred_tags_set1\" : str(article.keywords),                 # tags derived from article.keywords\n",
    "            \"inferred_tags_set2\" : {},                                    # tags inferred by our custom method\n",
    "        \n",
    "        }\n",
    "    \n",
    "    if len(data[\"news\"]) > 10 and len(data[\"title\"]) > 0:\n",
    "        data['stats']['num_char'] = len(data['news'])\n",
    "        \n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "        # if date =='None':\n",
    "        #     data['date'] = str(article.publish_date)\n",
    "        \n",
    "        tags = given_tags(item,data)\n",
    "        data['tags'] = tags\n",
    " \n",
    "        #src = source(item,data)\n",
    "        #data['source'] = src\n",
    "        \n",
    "        #src = source_1(item,data)\n",
    "        #data['source_1'] = src\n",
    "        \n",
    "        auth = author(item,data)\n",
    "        data['author'] = auth\n",
    "        \n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "    \n",
    "        \n",
    "    if len(data[\"title\"]) == 0:\n",
    "        headline = title(item,data)\n",
    "        data['title'] = headline\n",
    "    \n",
    "    if data['hostname'] in ['www.firesideguard.com']:\n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "    \n",
    "    if data['hostname'] in ['lewispnj.com', 'maconhomepress.com', 'www.theijnews.com']:\n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "        \n",
    "    if data['hostname'] in ['www.redlatinastl.com']:\n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "        \n",
    "    if data['hostname'] in ['maconhomepress.com', 'www.theijnews.com']:\n",
    "        auth = author(item,data)\n",
    "        data['author'] = auth\n",
    "        d\n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "    \n",
    "    if data['hostname'] == 'www.4bcaonline.com':\n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "        \n",
    "        date = given_date(item,data)\n",
    "        data['date'] = date\n",
    "        \n",
    "        auth = author(item,data)\n",
    "        data['author'] = auth\n",
    "        \n",
    "    if data['hostname'] == 'bocojo.com':\n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "        \n",
    "    if data['hostname'] == 'www.redlatinastl.com':\n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "        \n",
    "    if data['hostname'] == 'molawyersmedia.com':\n",
    "        nws = news(item,data)\n",
    "        data['news'] = nws\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_articles = []\n",
    "for article in raw_articles: \n",
    "    curated_article = process_article(article)\n",
    "    curated_articles.append(curated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
